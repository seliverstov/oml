{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=5,6,7\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=5,6,7\n",
    "import os\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import math \n",
    "\n",
    "from torchtext import data, datasets\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init cuda\n",
    "#device = \"cuda:6\" if torch.cuda.is_available() else \"cpu\"\n",
    "#idevice = 6 if torch.cuda.is_available() else -1\n",
    "#torch.cuda.set_device(idevice)\n",
    "#device, idevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field()\n",
    "PATH = Path('./wikitext/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "sequence_length = 10\n",
    "grad_clip = 0.1\n",
    "lr = 4.\n",
    "best_val_loss = None\n",
    "log_interval = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets.language_modeling import LanguageModelingDataset\n",
    "\n",
    "class WikiTextRu(LanguageModelingDataset):\n",
    "\n",
    "    urls = ['http://files.deeppavlov.ai/datasets/wikitext_ru.zip']\n",
    "    name = 'wikitext_ru'\n",
    "    dirname = 'wikitext_ru'\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, text_field, root='.data', \n",
    "               train='ru.wiki.train.txt', validation='ru.wiki.valid.txt', test='ru.wiki.test.txt',  **kwargs):\n",
    "\n",
    "        return super().splits(text_field=text_field, root=root, \n",
    "                              train=train, validation=validation, test=test, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def iters(cls, batch_size=32, bptt_len=35, device=0, root='.data', vectors=None, **kwargs):\n",
    "       \n",
    "        TEXT = data.Field()\n",
    "        train, val, test = cls.splits(TEXT, root=root, **kwargs)\n",
    "        TEXT.build_vocab(train, vectors=vectors)\n",
    "\n",
    "        return data.BPTTIterator.splits((train, val, test), \n",
    "                                        batch_size=batch_size, bptt_len=bptt_len, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import Vectors\n",
    "\n",
    "class RuFastText(Vectors):\n",
    "\n",
    "    url_base = 'http://files.deeppavlov.ai/embeddings/ft_native_300_ru_wiki_lenta_nltk_word_tokenize/ft_native_300_ru_wiki_lenta_nltk_word_tokenize.vec'\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        url = self.url_base\n",
    "        name = os.path.basename(url)\n",
    "        super().__init__(name, url=url, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 15s, sys: 18.8 s, total: 1min 33s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_ds, valid_ds, test_ds = WikiTextRu.splits(TEXT, root=PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_vectors = RuFastText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 15s, sys: 14.2 s, total: 2min 29s\n",
      "Wall time: 2min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TEXT.build_vocab(train_ds, min_freq=30, max_size=10_000, vectors=ru_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10002"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens = len(TEXT.vocab)\n",
    "ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 173 µs, sys: 33 µs, total: 206 µs\n",
      "Wall time: 213 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_loader, val_loader, test_loader = data.BPTTIterator.splits((train_ds, valid_ds, test_ds), \n",
    "                                        batch_sizes=(batch_size, batch_size, batch_size), \n",
    "                                        bptt_len=sequence_length, repeat=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, emb_vectors, nhid, nlayers, dropout=0.5):\n",
    "        super().__init__()\n",
    "        ntoken = emb_vectors.shape[0]\n",
    "        ninp = emb_vectors.shape[1]\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.encoder.weight.data.copy_(emb_vectors);\n",
    "        self.encoder.weight.requires_grad = False\n",
    "    \n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        # self.rnn = nn.DataParallel(self.rnn)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
    "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "        else:\n",
    "            return weight.new(self.nlayers, bsz, self.nhid).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(TEXT.vocab)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for i, b in enumerate(data_loader):\n",
    "        data, targets = b.text, b.target\n",
    "        output, hidden = model(data)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += criterion(output_flat, targets.view(-1)).item()\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    ntokens = len(TEXT.vocab) #len(corpus.dictionary)\n",
    "    for batch, b in enumerate(train_loader):\n",
    "        data, targets = b.text, b.target\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), grad_clip)\n",
    "        for p in filter(lambda p: p.requires_grad, model.parameters()):\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_loader), lr, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n=50, temp=1.):\n",
    "    model.eval()\n",
    "    x = torch.rand(1, 1).mul(ntokens).long().cuda()\n",
    "    hidden = None\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        output, hidden = model(x, hidden)\n",
    "        s_weights = output.squeeze().data.div(temp).exp()\n",
    "        s_idx = torch.multinomial(s_weights, 1)[0]\n",
    "        x.data.fill_(s_idx)\n",
    "        s = TEXT.vocab.itos[s_idx]\n",
    "        out.append(s)\n",
    "    return ' '.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_vectors = TEXT.vocab.vectors\n",
    "model = RNNModel('LSTM', emb_vectors, 128, 3, 0.3).cuda()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " белый релиза 1832 другими функций перешли севере участвовали оркестром 1875 легенде количеством Швейцария переехал интерес \n",
      "\n",
      "CPU times: user 14.3 ms, sys: 0 ns, total: 14.3 ms\n",
      "Wall time: 13.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    print('sample:\\n', generate(15), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 10000/37437 batches | lr 4.00 | loss  5.12 | ppl   166.86\n",
      "| epoch   1 | 20000/37437 batches | lr 4.00 | loss  4.68 | ppl   107.90\n",
      "| epoch   1 | 30000/37437 batches | lr 4.00 | loss  4.51 | ppl    90.91\n",
      "CPU times: user 16min 32s, sys: 5min 44s, total: 22min 16s\n",
      "Wall time: 22min 17s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss  4.27 | valid ppl    71.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " <unk> <unk> вопрос 22 <unk> <unk> Ирана <unk> По <unk> миру <eos> <unk> государством расположены <unk> <unk> показал в <unk> ; 100 проект был <unk> , хотя метод <unk> государственной \" <unk> <unk> \" , <unk> был произведён , который занял в состав <unk> <unk> с <unk> отделом их <unk> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 2):\n",
    "    %time train()\n",
    "    val_loss = evaluate(val_loader)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(50), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 10000/37437 batches | lr 4.00 | loss  4.37 | ppl    79.07\n",
      "| epoch   2 | 20000/37437 batches | lr 4.00 | loss  4.32 | ppl    75.53\n",
      "| epoch   2 | 30000/37437 batches | lr 4.00 | loss  4.29 | ppl    73.32\n",
      "CPU times: user 16min 33s, sys: 5min 47s, total: 22min 20s\n",
      "Wall time: 22min 19s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss  4.11 | valid ppl    61.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " наук и Дэвид государств 2 октября 2014 года. <unk> <unk> <unk> с <unk> на всех 1-й , ранее результате <unk> <unk> Таким образом о своей чемпионатах с войском в национальном <unk> поражений во время <unk> вторжения на основе место. Кубка сезонов за тем , что <unk> <unk> <unk> <unk> <unk> \n",
      "\n",
      "| epoch   3 | 10000/37437 batches | lr 4.00 | loss  4.26 | ppl    70.46\n",
      "| epoch   3 | 20000/37437 batches | lr 4.00 | loss  4.23 | ppl    69.03\n",
      "| epoch   3 | 30000/37437 batches | lr 4.00 | loss  4.22 | ppl    68.11\n",
      "CPU times: user 16min 28s, sys: 5min 50s, total: 22min 18s\n",
      "Wall time: 22min 18s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss  4.04 | valid ppl    57.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " жизни в <unk> и <unk> таких флота После распада <unk> <unk> <unk> <unk> стал продолжение в Институт годы , <unk> <unk> со <unk> народов <unk> республики достиг в <unk> ( <unk> ) <unk> пост со своими <unk> Псковской по <unk> части с <unk> <unk> <unk> <unk> и <unk> , а \n",
      "\n",
      "| epoch   4 | 10000/37437 batches | lr 4.00 | loss  4.20 | ppl    66.68\n",
      "| epoch   4 | 20000/37437 batches | lr 4.00 | loss  4.19 | ppl    65.81\n",
      "| epoch   4 | 30000/37437 batches | lr 4.00 | loss  4.18 | ppl    65.31\n",
      "CPU times: user 16min 26s, sys: 5min 44s, total: 22min 11s\n",
      "Wall time: 22min 11s\n",
      "| epoch   5 | 10000/37437 batches | lr 4.00 | loss  4.17 | ppl    64.48\n",
      "| epoch   5 | 20000/37437 batches | lr 4.00 | loss  4.16 | ppl    63.86\n",
      "| epoch   5 | 30000/37437 batches | lr 4.00 | loss  4.15 | ppl    63.57\n",
      "CPU times: user 16min 31s, sys: 5min 46s, total: 22min 17s\n",
      "Wall time: 22min 17s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss  3.97 | valid ppl    53.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " году 1912 <unk> <eos> <unk> <unk> <unk> <unk> <unk> становится <unk> в состав <unk> <unk> <unk> <unk> 8 февраля 2010 года и <unk> новую более 300 <unk> Это школа перестал , <unk> факторов партии , <unk> <unk> Также , завершился <unk> остатки , и <unk> на плане <unk> по с \n",
      "\n",
      "| epoch   6 | 10000/37437 batches | lr 4.00 | loss  4.14 | ppl    63.06\n",
      "| epoch   6 | 20000/37437 batches | lr 4.00 | loss  4.14 | ppl    62.57\n",
      "| epoch   6 | 30000/37437 batches | lr 4.00 | loss  4.13 | ppl    62.39\n",
      "CPU times: user 16min 36s, sys: 5min 40s, total: 22min 17s\n",
      "Wall time: 22min 17s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | valid loss  3.95 | valid ppl    52.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " <unk> с <unk> отрядом <unk> Ранее , а также <unk> денег и татар . <eos> <eos> В августе 1901 года <unk> <unk> вошла в построили <unk> Петра училище <unk> больницы <unk> <unk> <unk> <unk> В 1977 году Анна супругов Александр <unk> письмо , <unk> <unk> на основе реки единого , \n",
      "\n",
      "| epoch   7 | 10000/37437 batches | lr 4.00 | loss  4.13 | ppl    62.05\n",
      "| epoch   7 | 20000/37437 batches | lr 4.00 | loss  4.12 | ppl    61.66\n",
      "| epoch   7 | 30000/37437 batches | lr 4.00 | loss  4.12 | ppl    61.54\n",
      "CPU times: user 16min 45s, sys: 5min 45s, total: 22min 30s\n",
      "Wall time: 22min 30s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | valid loss  3.94 | valid ppl    51.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " игроки НКВД вместо <unk> , <unk> и <unk> , удалось с получения <unk> <unk> , и <unk> <unk> на свою <unk> характер <unk> <unk> . <eos> <eos> <unk> и <unk> каждому , <unk> над лишь <unk> сил . <eos> <eos> В начале <unk> <unk> в <unk> <unk> 1974 года , \n",
      "\n",
      "| epoch   8 | 10000/37437 batches | lr 4.00 | loss  4.12 | ppl    61.30\n",
      "| epoch   8 | 20000/37437 batches | lr 4.00 | loss  4.11 | ppl    60.95\n",
      "| epoch   8 | 30000/37437 batches | lr 4.00 | loss  4.11 | ppl    60.87\n",
      "CPU times: user 16min 48s, sys: 5min 44s, total: 22min 32s\n",
      "Wall time: 22min 32s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | valid loss  3.93 | valid ppl    50.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " Турции , был первым премьер-министром арене Рима <unk> командного , Хорватии — <unk> делами <unk> рабочих , <unk> от них селом , и в 1932 году в матче , <unk> на <unk> <unk> ) было <unk> в <unk> , а около 180 лет стал чемпионом Аргентины в борьбе чемпионата заняла \n",
      "\n",
      "| epoch   9 | 10000/37437 batches | lr 4.00 | loss  4.11 | ppl    60.71\n",
      "| epoch   9 | 20000/37437 batches | lr 4.00 | loss  4.10 | ppl    60.40\n",
      "| epoch   9 | 30000/37437 batches | lr 4.00 | loss  4.10 | ppl    60.34\n",
      "CPU times: user 16min 50s, sys: 5min 44s, total: 22min 34s\n",
      "Wall time: 22min 34s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | valid loss  3.92 | valid ppl    50.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " <unk> к <unk> <unk> <unk> , и <unk> <unk> течения и тщательно » <unk> . <eos> <eos> До 1887 1920-х <unk> и Роман <unk> <unk> в связи с <unk> <unk> I российским <unk> округа ( на скорость год ) . После этой войны учителем <unk> ( по годы немецкой династии \n",
      "\n",
      "| epoch  10 | 10000/37437 batches | lr 4.00 | loss  4.10 | ppl    60.22\n",
      "| epoch  10 | 20000/37437 batches | lr 4.00 | loss  4.09 | ppl    59.93\n",
      "| epoch  10 | 30000/37437 batches | lr 4.00 | loss  4.09 | ppl    59.90\n",
      "CPU times: user 16min 42s, sys: 5min 45s, total: 22min 27s\n",
      "Wall time: 22min 26s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | valid loss  3.91 | valid ppl    49.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " и среднего <unk> вузов 1-м <unk> ( 2 тысяч долларов ) , в <unk> в городе — <unk> <unk> <unk> в <unk> аэропорту Индии в <unk> уезда в семье <unk> <unk> В конце XX века на <unk> <unk> культурные <unk> <unk> этой улица , что <unk> не <unk> , чтобы \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2, 6):\n",
    "    %time train()\n",
    "    val_loss = evaluate(val_loader)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(50), '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
