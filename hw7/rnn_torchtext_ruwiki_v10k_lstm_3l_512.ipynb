{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import math \n",
    "\n",
    "from torchtext import data, datasets\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cuda:6', 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init cuda\n",
    "device = \"cuda:6\" if torch.cuda.is_available() else \"cpu\"\n",
    "idevice = 6 if torch.cuda.is_available() else -1\n",
    "torch.cuda.set_device(idevice)\n",
    "device, idevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field()\n",
    "PATH = Path('./wikitext/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "sequence_length = 30\n",
    "grad_clip = 0.1\n",
    "lr = 4.\n",
    "best_val_loss = None\n",
    "log_interval = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets.language_modeling import LanguageModelingDataset\n",
    "\n",
    "class WikiTextRu(LanguageModelingDataset):\n",
    "\n",
    "    urls = ['http://files.deeppavlov.ai/datasets/wikitext_ru.zip']\n",
    "    name = 'wikitext_ru'\n",
    "    dirname = 'wikitext_ru'\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, text_field, root='.data', \n",
    "               train='ru.wiki.train.txt', validation='ru.wiki.valid.txt', test='ru.wiki.test.txt',  **kwargs):\n",
    "\n",
    "        return super().splits(text_field=text_field, root=root, \n",
    "                              train=train, validation=validation, test=test, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def iters(cls, batch_size=32, bptt_len=35, device=0, root='.data', vectors=None, **kwargs):\n",
    "       \n",
    "        TEXT = data.Field()\n",
    "        train, val, test = cls.splits(TEXT, root=root, **kwargs)\n",
    "        TEXT.build_vocab(train, vectors=vectors)\n",
    "\n",
    "        return data.BPTTIterator.splits((train, val, test), \n",
    "                                        batch_size=batch_size, bptt_len=bptt_len, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import Vectors\n",
    "\n",
    "class RuFastText(Vectors):\n",
    "\n",
    "    url_base = 'http://files.deeppavlov.ai/embeddings/ft_native_300_ru_wiki_lenta_nltk_word_tokenize/ft_native_300_ru_wiki_lenta_nltk_word_tokenize.vec'\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        url = self.url_base\n",
    "        name = os.path.basename(url)\n",
    "        super().__init__(name, url=url, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 22.4 s, total: 1min 45s\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_ds, valid_ds, test_ds = WikiTextRu.splits(TEXT, root=PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_vectors = RuFastText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 55s, sys: 18.7 s, total: 3min 14s\n",
      "Wall time: 3min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TEXT.build_vocab(train_ds, min_freq=30, max_size=10_000, vectors=ru_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10002"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens = len(TEXT.vocab)\n",
    "ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 163 µs, sys: 31 µs, total: 194 µs\n",
      "Wall time: 201 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_loader, val_loader, test_loader = data.BPTTIterator.splits((train_ds, valid_ds, test_ds), \n",
    "                                        batch_sizes=(batch_size, batch_size, batch_size), \n",
    "                                        bptt_len=sequence_length, repeat=False, device=idevice) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, emb_vectors, nhid, nlayers, dropout=0.5):\n",
    "        super().__init__()\n",
    "        ntoken = emb_vectors.shape[0]\n",
    "        ninp = emb_vectors.shape[1]\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.encoder.weight.data.copy_(emb_vectors);\n",
    "        self.encoder.weight.requires_grad = False\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
    "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "        else:\n",
    "            return weight.new(self.nlayers, bsz, self.nhid).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(TEXT.vocab)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for i, b in enumerate(data_loader):\n",
    "        data, targets = b.text, b.target\n",
    "        output, hidden = model(data)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += len(data) * criterion(output_flat, targets.view(-1)).item()\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    ntokens = len(TEXT.vocab) #len(corpus.dictionary)\n",
    "    for batch, b in enumerate(train_loader):\n",
    "        data, targets = b.text, b.target\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), grad_clip)\n",
    "        for p in filter(lambda p: p.requires_grad, model.parameters()):\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_loader), lr, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n=50, temp=1.):\n",
    "    model.eval()\n",
    "    x = torch.rand(1, 1).mul(ntokens).long().to(device)\n",
    "    hidden = None\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        output, hidden = model(x, hidden)\n",
    "        s_weights = output.squeeze().data.div(temp).exp()\n",
    "        s_idx = torch.multinomial(s_weights, 1)[0]\n",
    "        x.data.fill_(s_idx)\n",
    "        s = TEXT.vocab.itos[s_idx]\n",
    "        out.append(s)\n",
    "    return ' '.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_vectors = TEXT.vocab.vectors\n",
    "model = RNNModel('LSTM', emb_vectors, 512, 3, 0.3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " исследованиях познакомился турецкой der обозначена горного сюжета отца. осады написанных 40 горных 21 Начало крепости \n",
      "\n",
      "CPU times: user 16.9 ms, sys: 161 µs, total: 17 ms\n",
      "Wall time: 16.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    print('sample:\\n', generate(15), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 10000/49916 batches | lr 4.00 | loss  5.03 | ppl   152.49\n",
      "| epoch   1 | 20000/49916 batches | lr 4.00 | loss  4.49 | ppl    89.01\n",
      "| epoch   1 | 30000/49916 batches | lr 4.00 | loss  4.28 | ppl    72.16\n",
      "| epoch   1 | 40000/49916 batches | lr 4.00 | loss  4.15 | ppl    63.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss 116.98 | valid ppl 635378789003516024374430337499170846140141506920448.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " произведений , <unk> победой спорта <unk> факультета и снялся <unk> <unk> <unk> фигур <unk> В 1953 г. <unk> <unk> <unk> 2 захватить информации уездного <unk> за свой <unk> Сейчас , белые <unk> <unk> владений <unk> <unk> , <unk> <unk> и семья . <eos> <eos> В январе 2012 года во время \n",
      "\n",
      "| epoch   2 | 10000/49916 batches | lr 4.00 | loss  4.00 | ppl    54.40\n",
      "| epoch   2 | 20000/49916 batches | lr 4.00 | loss  3.94 | ppl    51.41\n",
      "| epoch   2 | 30000/49916 batches | lr 4.00 | loss  3.90 | ppl    49.44\n",
      "| epoch   2 | 40000/49916 batches | lr 4.00 | loss  3.86 | ppl    47.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss 109.87 | valid ppl 520063241929235020174999283892860050672608870400.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " — 1915 и 1922 годах был одним из театральных организации партии и промышленности. каменный реальных Ленинграда — артист по британской из Италии. значительных художников книги . <eos> Участвовал при <unk> с <unk> , экономических недолго в распределение и пользуется год жил в студии <unk> , к концу 1990-х годов и \n",
      "\n",
      "| epoch   3 | 10000/49916 batches | lr 4.00 | loss  3.81 | ppl    44.97\n",
      "| epoch   3 | 20000/49916 batches | lr 4.00 | loss  3.78 | ppl    43.72\n",
      "| epoch   3 | 30000/49916 batches | lr 4.00 | loss  3.76 | ppl    42.97\n",
      "| epoch   3 | 40000/49916 batches | lr 4.00 | loss  3.74 | ppl    42.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss 106.36 | valid ppl 15538927415002850975164519700999000995183722496.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " 2014 года <unk> , <unk> в 135 , а также <unk> <unk> <unk> с участием чёрной <unk> от <unk> <unk> » . В <unk> 2011 году <unk> <unk> <unk> <unk> фамилию на новую <unk> <unk> , <unk> полной между <unk> <unk> постоянным при выполнении юридических <unk> и обеспечения политических <unk> \n",
      "\n",
      "| epoch   4 | 10000/49916 batches | lr 4.00 | loss  3.71 | ppl    40.81\n",
      "| epoch   4 | 20000/49916 batches | lr 4.00 | loss  3.69 | ppl    40.10\n",
      "| epoch   4 | 30000/49916 batches | lr 4.00 | loss  3.68 | ppl    39.77\n",
      "| epoch   4 | 40000/49916 batches | lr 4.00 | loss  3.67 | ppl    39.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss 104.33 | valid ppl 2041879480776724864468182925879620201000992768.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " на <unk> <unk> <unk> и <unk> <unk> , весьма чемпионата за его <unk> : <unk> <unk> на <unk> <unk> на <unk> <unk> и <unk> и <unk> , их <unk> и <unk> , <unk> , <unk> и <unk> Он взял в совет и поручил <unk> <unk> В <unk> игре <unk> <unk> \n",
      "\n",
      "| epoch   5 | 10000/49916 batches | lr 4.00 | loss  3.65 | ppl    38.52\n",
      "| epoch   5 | 20000/49916 batches | lr 4.00 | loss  3.64 | ppl    38.03\n",
      "| epoch   5 | 30000/49916 batches | lr 4.00 | loss  3.63 | ppl    37.89\n",
      "| epoch   5 | 40000/49916 batches | lr 4.00 | loss  3.63 | ppl    37.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss 103.04 | valid ppl 561255808902636679500054764722272756058554368.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " года завершил чемпионат Европы . <eos> <eos> <unk> — воспитанник клуба <unk> » . <eos> <eos> <unk> произошло карьеру в клубе <unk> » , <unk> Воспитанник <unk> . <eos> <eos> В 1989 году поступил на военную службу на <unk> <unk> » , где попал в состав клуба. . <eos> <eos> \n",
      "\n",
      "| epoch   6 | 10000/49916 batches | lr 4.00 | loss  3.61 | ppl    37.06\n",
      "| epoch   6 | 20000/49916 batches | lr 4.00 | loss  3.60 | ppl    36.69\n",
      "| epoch   6 | 30000/49916 batches | lr 4.00 | loss  3.60 | ppl    36.63\n",
      "| epoch   6 | 40000/49916 batches | lr 4.00 | loss  3.59 | ppl    36.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | valid loss 102.12 | valid ppl 224755164669026259174154608889443866262896640.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " суда . <eos> <eos> В крупных городах приняли участие 20 % населения : 1 ( 84 % ) и <unk> ( Россия . <eos> <eos> В 2002 году , после реформы <unk> — <unk> , <unk> <unk> раздел администрации так называемый <unk> дело Казахстана » . <unk> <unk> <unk> является \n",
      "\n",
      "| epoch   7 | 10000/49916 batches | lr 4.00 | loss  3.58 | ppl    36.02\n",
      "| epoch   7 | 20000/49916 batches | lr 4.00 | loss  3.58 | ppl    35.72\n",
      "| epoch   7 | 30000/49916 batches | lr 4.00 | loss  3.58 | ppl    35.71\n",
      "| epoch   7 | 40000/49916 batches | lr 4.00 | loss  3.57 | ppl    35.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | valid loss 101.47 | valid ppl 117262741365009719867396163720675866833846272.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " , а в то время как <unk> , как первые , <unk> на <unk> и отдельные улучшения в <unk> <unk> Очень <unk> <unk> на основе <unk> <unk> <unk> существовало изменение <unk> клеток <unk> <unk> или <unk> управления <unk> при этом происходит от <unk> <unk> При <unk> этой структуры после <unk> \n",
      "\n",
      "| epoch   8 | 10000/49916 batches | lr 4.00 | loss  3.56 | ppl    35.23\n",
      "| epoch   8 | 20000/49916 batches | lr 4.00 | loss  3.55 | ppl    34.97\n",
      "| epoch   8 | 30000/49916 batches | lr 4.00 | loss  3.56 | ppl    35.00\n",
      "| epoch   8 | 40000/49916 batches | lr 4.00 | loss  3.55 | ppl    34.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | valid loss 100.96 | valid ppl 69981050707657235547109473558684654469382144.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " бассейна реки . <eos> <eos> Известен <unk> <unk> <unk> ( <unk> + ) , а также применяются <unk> <unk> от <unk> <unk> <unk> <unk> , а <unk> <unk> ( <unk> ) с <unk> <unk> <unk> в настоящее время расположено на юге <unk> сельского поселения , где они находятся в различных \n",
      "\n",
      "| epoch   9 | 10000/49916 batches | lr 4.00 | loss  3.54 | ppl    34.60\n",
      "| epoch   9 | 20000/49916 batches | lr 4.00 | loss  3.54 | ppl    34.37\n",
      "| epoch   9 | 30000/49916 batches | lr 4.00 | loss  3.54 | ppl    34.41\n",
      "| epoch   9 | 40000/49916 batches | lr 4.00 | loss  3.53 | ppl    34.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | valid loss 100.45 | valid ppl 42136761772461030479719970266848426069065728.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " <eos> <eos> <unk> получил <eos> <eos> <unk> <unk> Иисуса Христа — <unk> ( <unk> отца <unk> людей ) и все события в <unk> <unk> с особой <unk> <unk> Президент поэзии от <unk> назывались <unk> . <eos> <eos> <unk> : три роты Федерального ( <unk> и иностранные ) запасы <unk> требования \n",
      "\n",
      "| epoch  10 | 10000/49916 batches | lr 4.00 | loss  3.53 | ppl    34.07\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "for epoch in range(1, 11):\n",
    "    train()\n",
    "    val_loss = evaluate(val_loader)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(50), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
