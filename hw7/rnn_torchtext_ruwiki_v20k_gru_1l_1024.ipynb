{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import math \n",
    "\n",
    "from torchtext import data, datasets\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cuda:6', 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init cuda\n",
    "device = \"cuda:6\" if torch.cuda.is_available() else \"cpu\"\n",
    "idevice = 6 if torch.cuda.is_available() else -1\n",
    "torch.cuda.set_device(idevice)\n",
    "device, idevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field()\n",
    "PATH = Path('./wikitext/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "sequence_length = 10\n",
    "grad_clip = 0.1\n",
    "lr = 4.\n",
    "best_val_loss = None\n",
    "log_interval = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets.language_modeling import LanguageModelingDataset\n",
    "\n",
    "class WikiTextRu(LanguageModelingDataset):\n",
    "\n",
    "    urls = ['http://files.deeppavlov.ai/datasets/wikitext_ru.zip']\n",
    "    name = 'wikitext_ru'\n",
    "    dirname = 'wikitext_ru'\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, text_field, root='.data', \n",
    "               train='ru.wiki.train.txt', validation='ru.wiki.valid.txt', test='ru.wiki.test.txt',  **kwargs):\n",
    "\n",
    "        return super().splits(text_field=text_field, root=root, \n",
    "                              train=train, validation=validation, test=test, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def iters(cls, batch_size=32, bptt_len=35, device=0, root='.data', vectors=None, **kwargs):\n",
    "       \n",
    "        TEXT = data.Field()\n",
    "        train, val, test = cls.splits(TEXT, root=root, **kwargs)\n",
    "        TEXT.build_vocab(train, vectors=vectors)\n",
    "\n",
    "        return data.BPTTIterator.splits((train, val, test), \n",
    "                                        batch_size=batch_size, bptt_len=bptt_len, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import Vectors\n",
    "\n",
    "class RuFastText(Vectors):\n",
    "\n",
    "    url_base = 'http://files.deeppavlov.ai/embeddings/ft_native_300_ru_wiki_lenta_nltk_word_tokenize/ft_native_300_ru_wiki_lenta_nltk_word_tokenize.vec'\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        url = self.url_base\n",
    "        name = os.path.basename(url)\n",
    "        super().__init__(name, url=url, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 25s, sys: 23.8 s, total: 1min 49s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_ds, valid_ds, test_ds = WikiTextRu.splits(TEXT, root=PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_vectors = RuFastText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 53s, sys: 19.2 s, total: 3min 12s\n",
      "Wall time: 3min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TEXT.build_vocab(train_ds, min_freq=30, max_size=20_000, vectors=ru_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20002"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens = len(TEXT.vocab)\n",
    "ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 153 µs, sys: 30 µs, total: 183 µs\n",
      "Wall time: 190 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_loader, val_loader, test_loader = data.BPTTIterator.splits((train_ds, valid_ds, test_ds), \n",
    "                                        batch_sizes=(batch_size, batch_size, batch_size), \n",
    "                                        bptt_len=sequence_length, repeat=False, device=idevice) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, emb_vectors, nhid, nlayers, dropout=0.5):\n",
    "        super().__init__()\n",
    "        ntoken = emb_vectors.shape[0]\n",
    "        ninp = emb_vectors.shape[1]\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.encoder.weight.data.copy_(emb_vectors);\n",
    "        self.encoder.weight.requires_grad = False\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
    "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "        else:\n",
    "            return weight.new(self.nlayers, bsz, self.nhid).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(TEXT.vocab)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for i, b in enumerate(data_loader):\n",
    "        data, targets = b.text, b.target\n",
    "        output, hidden = model(data)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += criterion(output_flat, targets.view(-1)).item()\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    ntokens = len(TEXT.vocab) #len(corpus.dictionary)\n",
    "    for batch, b in enumerate(train_loader):\n",
    "        data, targets = b.text, b.target\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), grad_clip)\n",
    "        for p in filter(lambda p: p.requires_grad, model.parameters()):\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_loader), lr, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n=50, temp=1.):\n",
    "    model.eval()\n",
    "    x = torch.rand(1, 1).mul(ntokens).long().to(device)\n",
    "    hidden = None\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        output, hidden = model(x, hidden)\n",
    "        s_weights = output.squeeze().data.div(temp).exp()\n",
    "        s_idx = torch.multinomial(s_weights, 1)[0]\n",
    "        x.data.fill_(s_idx)\n",
    "        s = TEXT.vocab.itos[s_idx]\n",
    "        out.append(s)\n",
    "    return ' '.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_vectors = TEXT.vocab.vectors\n",
    "model = RNNModel('GRU', emb_vectors, 1024, 1, 0.0).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " направился мимо моей орденов журналы степной за началось песне документ дорог Португалии. погиб Екатерина естественным \n",
      "\n",
      "CPU times: user 13.2 ms, sys: 4.24 ms, total: 17.4 ms\n",
      "Wall time: 16.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    print('sample:\\n', generate(15), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 10000/149747 batches | lr 4.00 | loss  5.58 | ppl   265.13\n",
      "| epoch   1 | 20000/149747 batches | lr 4.00 | loss  5.10 | ppl   164.06\n",
      "| epoch   1 | 30000/149747 batches | lr 4.00 | loss  4.90 | ppl   133.73\n",
      "| epoch   1 | 40000/149747 batches | lr 4.00 | loss  4.77 | ppl   117.46\n",
      "| epoch   1 | 50000/149747 batches | lr 4.00 | loss  4.68 | ppl   108.16\n",
      "| epoch   1 | 60000/149747 batches | lr 4.00 | loss  4.61 | ppl   100.02\n",
      "| epoch   1 | 70000/149747 batches | lr 4.00 | loss  4.56 | ppl    95.68\n",
      "| epoch   1 | 80000/149747 batches | lr 4.00 | loss  4.51 | ppl    91.12\n",
      "| epoch   1 | 90000/149747 batches | lr 4.00 | loss  4.46 | ppl    86.83\n",
      "| epoch   1 | 100000/149747 batches | lr 4.00 | loss  4.43 | ppl    83.93\n",
      "| epoch   1 | 110000/149747 batches | lr 4.00 | loss  4.40 | ppl    81.85\n",
      "| epoch   1 | 120000/149747 batches | lr 4.00 | loss  4.38 | ppl    79.81\n",
      "| epoch   1 | 130000/149747 batches | lr 4.00 | loss  4.36 | ppl    78.14\n",
      "| epoch   1 | 140000/149747 batches | lr 4.00 | loss  4.32 | ppl    75.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss  4.31 | valid ppl    74.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " , не <unk> <unk> ударами . <eos> Фильм <unk> региональных обнаружила многих с <unk> и <unk> <unk> приблизительно . <eos> <eos> <eos> С 1967 <unk> <unk> с языка <unk> является <unk> , воин из которых Первая <unk> <unk> и <unk> <unk> при <unk> не <unk> <unk> в виде Поэтому <unk> \n",
      "\n",
      "CPU times: user 1h 37min 56s, sys: 40min 26s, total: 2h 18min 22s\n",
      "Wall time: 2h 18min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1, 2):\n",
    "    train()\n",
    "    val_loss = evaluate(val_loader)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(50), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 10000/149747 batches | lr 4.00 | loss  4.29 | ppl    72.91\n",
      "| epoch   2 | 20000/149747 batches | lr 4.00 | loss  4.27 | ppl    71.49\n",
      "| epoch   2 | 30000/149747 batches | lr 4.00 | loss  4.25 | ppl    70.18\n",
      "| epoch   2 | 40000/149747 batches | lr 4.00 | loss  4.24 | ppl    69.07\n",
      "| epoch   2 | 50000/149747 batches | lr 4.00 | loss  4.23 | ppl    68.51\n",
      "| epoch   2 | 60000/149747 batches | lr 4.00 | loss  4.20 | ppl    66.88\n",
      "| epoch   2 | 70000/149747 batches | lr 4.00 | loss  4.20 | ppl    66.93\n",
      "| epoch   2 | 80000/149747 batches | lr 4.00 | loss  4.19 | ppl    66.17\n",
      "| epoch   2 | 90000/149747 batches | lr 4.00 | loss  4.17 | ppl    64.95\n",
      "| epoch   2 | 100000/149747 batches | lr 4.00 | loss  4.16 | ppl    64.31\n",
      "| epoch   2 | 110000/149747 batches | lr 4.00 | loss  4.16 | ppl    64.10\n",
      "| epoch   2 | 120000/149747 batches | lr 4.00 | loss  4.15 | ppl    63.65\n",
      "| epoch   2 | 130000/149747 batches | lr 4.00 | loss  4.15 | ppl    63.37\n",
      "| epoch   2 | 140000/149747 batches | lr 4.00 | loss  4.13 | ppl    61.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss  4.14 | valid ppl    62.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " домашней <unk> <unk> <unk> ( штаты % — в 1998 ) , но не точности в 1-м . <eos> <eos> В 1925 году Евгений его дочерей <unk> \" <unk> \" открыла <unk> с <unk> <unk> , которая <unk> на японском языке . <eos> <eos> <unk> Dream был заново <unk> числу \n",
      "\n",
      "| epoch   3 | 10000/149747 batches | lr 4.00 | loss  4.12 | ppl    61.36\n",
      "| epoch   3 | 20000/149747 batches | lr 4.00 | loss  4.11 | ppl    60.79\n",
      "| epoch   3 | 30000/149747 batches | lr 4.00 | loss  4.10 | ppl    60.19\n",
      "| epoch   3 | 40000/149747 batches | lr 4.00 | loss  4.09 | ppl    59.78\n",
      "| epoch   3 | 50000/149747 batches | lr 4.00 | loss  4.09 | ppl    59.73\n",
      "| epoch   3 | 60000/149747 batches | lr 4.00 | loss  4.07 | ppl    58.70\n",
      "| epoch   3 | 70000/149747 batches | lr 4.00 | loss  4.08 | ppl    59.11\n",
      "| epoch   3 | 80000/149747 batches | lr 4.00 | loss  4.07 | ppl    58.79\n",
      "| epoch   3 | 100000/149747 batches | lr 4.00 | loss  4.06 | ppl    57.71\n",
      "| epoch   3 | 110000/149747 batches | lr 4.00 | loss  4.06 | ppl    57.78\n",
      "| epoch   3 | 120000/149747 batches | lr 4.00 | loss  4.05 | ppl    57.60\n",
      "| epoch   3 | 130000/149747 batches | lr 4.00 | loss  4.05 | ppl    57.57\n",
      "| epoch   3 | 140000/149747 batches | lr 4.00 | loss  4.03 | ppl    56.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss  4.06 | valid ppl    57.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " и в горных <unk> районы значительно <unk> и <unk> тяжёлой для <unk> <unk> Это позволяет объяснить из <unk> с <unk> <unk> из <unk> <unk> груза <unk> остаётся быстро <unk> сквозь - <unk> <unk> Ширина не позволяет более высокой точности к <unk> При приближении формируются <unk> состава главным <unk> , как \n",
      "\n",
      "| epoch   4 | 10000/149747 batches | lr 4.00 | loss  4.03 | ppl    56.31\n",
      "| epoch   4 | 20000/149747 batches | lr 4.00 | loss  4.02 | ppl    55.95\n",
      "| epoch   4 | 30000/149747 batches | lr 4.00 | loss  4.02 | ppl    55.54\n",
      "| epoch   4 | 40000/149747 batches | lr 4.00 | loss  4.01 | ppl    55.32\n",
      "| epoch   4 | 50000/149747 batches | lr 4.00 | loss  4.01 | ppl    55.41\n",
      "| epoch   4 | 60000/149747 batches | lr 4.00 | loss  4.00 | ppl    54.58\n",
      "| epoch   4 | 70000/149747 batches | lr 4.00 | loss  4.01 | ppl    55.09\n",
      "| epoch   4 | 80000/149747 batches | lr 4.00 | loss  4.01 | ppl    54.92\n",
      "| epoch   4 | 90000/149747 batches | lr 4.00 | loss  3.99 | ppl    54.31\n",
      "| epoch   4 | 100000/149747 batches | lr 4.00 | loss  3.99 | ppl    54.11\n",
      "| epoch   4 | 110000/149747 batches | lr 4.00 | loss  3.99 | ppl    54.28\n",
      "| epoch   4 | 130000/149747 batches | lr 4.00 | loss  3.99 | ppl    54.26\n",
      "| epoch   4 | 140000/149747 batches | lr 4.00 | loss  3.98 | ppl    53.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss  4.00 | valid ppl    54.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " в истории <unk> , « » , <unk> » , <unk> » , <unk> » , <unk> » . Вслед в его современников работах называют <unk> работы \" <unk> <unk> , <unk> \" <unk> \" <unk> некоторые лодки позволяют <unk> трижды ( <unk> ) . В отличие от <unk> субъекта \n",
      "\n",
      "| epoch   5 | 10000/149747 batches | lr 4.00 | loss  3.98 | ppl    53.30\n",
      "| epoch   5 | 20000/149747 batches | lr 4.00 | loss  3.97 | ppl    53.04\n",
      "| epoch   5 | 30000/149747 batches | lr 4.00 | loss  3.96 | ppl    52.71\n",
      "| epoch   5 | 40000/149747 batches | lr 4.00 | loss  3.96 | ppl    52.58\n",
      "| epoch   5 | 50000/149747 batches | lr 4.00 | loss  3.97 | ppl    52.73\n",
      "| epoch   5 | 60000/149747 batches | lr 4.00 | loss  3.95 | ppl    52.01\n",
      "| epoch   5 | 70000/149747 batches | lr 4.00 | loss  3.96 | ppl    52.55\n",
      "| epoch   5 | 80000/149747 batches | lr 4.00 | loss  3.96 | ppl    52.44\n",
      "| epoch   5 | 90000/149747 batches | lr 4.00 | loss  3.95 | ppl    51.92\n",
      "| epoch   5 | 100000/149747 batches | lr 4.00 | loss  3.95 | ppl    51.78\n",
      "| epoch   5 | 110000/149747 batches | lr 4.00 | loss  3.95 | ppl    51.98\n",
      "| epoch   5 | 120000/149747 batches | lr 4.00 | loss  3.95 | ppl    51.95\n",
      "| epoch   5 | 130000/149747 batches | lr 4.00 | loss  3.95 | ppl    52.06\n",
      "| epoch   5 | 140000/149747 batches | lr 4.00 | loss  3.93 | ppl    51.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss  3.97 | valid ppl    52.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " ) , где 14 сентября 1879 года женился на <unk> 1847 . <eos> <eos> 5 июня 1944 года назначен первым заместителем Борисом <unk> , 1 мая под руководство была <unk> с войсками ВМС США . <eos> <eos> В 1967 г. в Южной <unk> был назначен <unk> для поручений в Вооружённых \n",
      "\n",
      "CPU times: user 6h 31min 24s, sys: 2h 41min 35s, total: 9h 13min\n",
      "Wall time: 9h 12min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(2, 6):\n",
    "    train()\n",
    "    val_loss = evaluate(val_loader)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(50), '\\n')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
