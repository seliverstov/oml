{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import math \n",
    "\n",
    "from torchtext import data, datasets\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "#import utils\n",
    "#import wiki_utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init cuda\n",
    "device = \"cuda:6\" if torch.cuda.is_available() else \"cpu\"\n",
    "idevice = 6 if torch.cuda.is_available() else -1\n",
    "torch.cuda.set_device(idevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = wiki_utils.Texts('./wikitext/')\n",
    "TEXT = data.Field(sequential=True, tokenize=lambda x : list(x))\n",
    "PATH = Path('./wikitext/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "sequence_length = 30\n",
    "grad_clip = 0.1\n",
    "lr = 4.\n",
    "best_val_loss = None\n",
    "log_interval = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, valid_ds, test_ds = datasets.WikiText2.splits(TEXT, root=PATH)\n",
    "#train_loader = wiki_utils.TextLoader(corpus.train, batch_size=batch_size)\n",
    "#val_loader = wiki_utils.TextLoader(corpus.valid, batch_size=eval_batch_size)\n",
    "#test_loader = wiki_utils.TextLoader(corpus.test, batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch_size = 128\n",
    "train_loader, val_loader, test_loader = data.BPTTIterator.splits((train_ds, valid_ds, test_ds), \n",
    "                                        batch_sizes=(batch_size, eval_batch_size, eval_batch_size), \n",
    "                                        bptt_len=sequence_length, repeat=False, device=idevice) \n",
    "#wiki_utils.TextLoader(corpus.train, batch_size=batch_size)\n",
    "#wiki_utils.TextLoader(corpus.valid, batch_size=eval_batch_size) \n",
    "#wiki_utils.TextLoader(corpus.test, batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
    "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "        else:\n",
    "            return weight.new(self.nlayers, bsz, self.nhid).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(TEXT.vocab) #len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for i, b in enumerate(data_loader):\n",
    "        data, targets = b.text, b.target\n",
    "        output, hidden = model(data)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += len(data) * criterion(output_flat, targets.view(-1)).item()\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    ntokens = len(TEXT.vocab) #len(corpus.dictionary)\n",
    "    for batch, b in enumerate(train_loader):\n",
    "        data, targets = b.text, b.target\n",
    "    #for batch, i in enumerate(range(0, train_data.size(0) - 1, sequence_length)):\n",
    "        #data, targets = get_batch(train_data, i)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_loader), lr, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(TEXT.vocab) #len(corpus.dictionary)\n",
    "model = RNNModel('LSTM', ntokens, 128, 128, 2, 0.3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n=50, temp=1.):\n",
    "    model.eval()\n",
    "    x = torch.rand(1, 1).mul(ntokens).long().to(device)\n",
    "    hidden = None\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        output, hidden = model(x, hidden)\n",
    "        s_weights = output.squeeze().data.div(temp).exp()\n",
    "        s_idx = torch.multinomial(s_weights, 1)[0]\n",
    "        x.data.fill_(s_idx)\n",
    "        s = TEXT.vocab.itos[s_idx] #corpus.dictionary.idx2symbol[s_idx]\n",
    "        out.append(s)\n",
    "    return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " LTń♭ვჯრÅწāGoن=.Ü>ë7صリリL,*ო3đ﻿–1áდò²!†戦iv>áīXyøAプ჻— \n",
      "\n",
      "| epoch   1 |  1000/ 2808 batches | lr 4.00 | loss  3.12 | ppl    22.62\n",
      "| epoch   1 |  2000/ 2808 batches | lr 4.00 | loss  2.44 | ppl    11.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss 58.52 | valid ppl 26109660363823811915677696.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  the Sphlartaar of on the Boflent andore bysurat a \n",
      "\n",
      "| epoch   2 |  1000/ 2808 batches | lr 4.00 | loss  2.06 | ppl     7.85\n",
      "| epoch   2 |  2000/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss 50.20 | valid ppl 6332017721268421787648.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  becine yeat find to the roo approse Grees . The < \n",
      "\n",
      "| epoch   3 |  1000/ 2808 batches | lr 4.00 | loss  1.86 | ppl     6.40\n",
      "| epoch   3 |  2000/ 2808 batches | lr 4.00 | loss  1.82 | ppl     6.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss 47.03 | valid ppl 266359478409525690368.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " eer on Lalgane and those for head in the varoter a \n",
      "\n",
      "| epoch   4 |  1000/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.89\n",
      "| epoch   4 |  2000/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss 45.54 | valid ppl 59729098715901919232.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " lusing other <unk> Talliduary . <unk> Astical Ale  \n",
      "\n",
      "| epoch   5 |  1000/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.63\n",
      "| epoch   5 |  2000/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss 44.63 | valid ppl 24066171901161897984.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  believed bean are other late Aircular Rinceland a \n",
      "\n",
      "| epoch   6 |  1000/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.47\n",
      "| epoch   6 |  2000/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | valid loss 44.00 | valid ppl 12840446981996128256.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " roat observing of Government , which sultrain that \n",
      "\n",
      "| epoch   7 |  1000/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.36\n",
      "| epoch   7 |  2000/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | valid loss 43.54 | valid ppl 8124554909277950976.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " adeled , life in a victory , and story out that th \n",
      "\n",
      "| epoch   8 |  1000/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.28\n",
      "| epoch   8 |  2000/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | valid loss 43.21 | valid ppl 5810107525754928128.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " ed the 8th was independence from core and have bee \n",
      "\n",
      "| epoch   9 |  1000/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.22\n",
      "| epoch   9 |  2000/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | valid loss 42.91 | valid ppl 4331652659799189504.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " -@ noted all the under the philosier air verriex a \n",
      "\n",
      "| epoch  10 |  1000/ 2808 batches | lr 4.00 | loss  1.64 | ppl     5.16\n",
      "| epoch  10 |  2000/ 2808 batches | lr 4.00 | loss  1.64 | ppl     5.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | valid loss 42.66 | valid ppl 3366361902029766656.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " andons . He was <unk> and music blights of A 's in \n",
      "\n",
      "| epoch  11 |  1000/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.12\n",
      "| epoch  11 |  2000/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | valid loss 42.50 | valid ppl 2854283353968859136.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " essaule researchestest played , which June 194 , t \n",
      "\n",
      "| epoch  12 |  1000/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.09\n",
      "| epoch  12 |  2000/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | valid loss 42.30 | valid ppl 2341083163796518400.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " aturided 's Unownswive \" that <unk> in <unk> Vodon \n",
      "\n",
      "| epoch  13 |  1000/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.05\n",
      "| epoch  13 |  2000/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | valid loss 42.15 | valid ppl 2028174246148133632.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  7 in June 1976 . <eos> In 1994 , and and new site , w \n",
      "\n",
      "| epoch  14 |  1000/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.03\n",
      "| epoch  14 |  2000/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | valid loss 42.02 | valid ppl 1768747891601582336.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  pick of publishing were works . <unk> Base Center \n",
      "\n",
      "| epoch  15 |  1000/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.01\n",
      "| epoch  15 |  2000/ 2808 batches | lr 4.00 | loss  1.61 | ppl     4.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | valid loss 41.92 | valid ppl 1610007196897986560.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  Showleban — 68 second next with Dien Wham Unicled \n",
      "\n",
      "| epoch  16 |  1000/ 2808 batches | lr 4.00 | loss  1.61 | ppl     4.98\n",
      "| epoch  16 |  2000/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | valid loss 41.78 | valid ppl 1393317131789745408.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  Voust ' moon of kill new intervot in Lery few sib \n",
      "\n",
      "| epoch  17 |  1000/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.96\n",
      "| epoch  17 |  2000/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | valid loss 41.71 | valid ppl 1305218730718009600.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " re declaring around 1406 . Landing Girnrama Leaker \n",
      "\n",
      "| epoch  18 |  1000/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.95\n",
      "| epoch  18 |  2000/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | valid loss 41.62 | valid ppl 1193698044784088064.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  control scientific that his birth – <unk> for the \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  19 |  1000/ 2808 batches | lr 4.00 | loss  1.59 | ppl     4.93\n",
      "| epoch  19 |  2000/ 2808 batches | lr 4.00 | loss  1.59 | ppl     4.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | valid loss 41.54 | valid ppl 1101952905907838464.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  . <eos> The regular stable months was conception exam \n",
      "\n",
      "| epoch  20 |  1000/ 2808 batches | lr 4.00 | loss  1.59 | ppl     4.91\n",
      "| epoch  20 |  2000/ 2808 batches | lr 4.00 | loss  1.59 | ppl     4.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | valid loss 41.47 | valid ppl 1023033142110551424.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " an indostression and top . The south , comparation \n",
      "\n",
      "| epoch  21 |  1000/ 2808 batches | lr 4.00 | loss  1.59 | ppl     4.90\n",
      "| epoch  21 |  2000/ 2808 batches | lr 4.00 | loss  1.59 | ppl     4.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | valid loss 41.41 | valid ppl 962036204216346880.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  policed decided such the currence \" , and the rel \n",
      "\n",
      "| epoch  22 |  1000/ 2808 batches | lr 4.00 | loss  1.59 | ppl     4.89\n",
      "| epoch  22 |  2000/ 2808 batches | lr 4.00 | loss  1.58 | ppl     4.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | valid loss 41.36 | valid ppl 912754939012453248.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  must for a north ( diecton and Hawai , transformi \n",
      "\n",
      "| epoch  23 |  1000/ 2808 batches | lr 4.00 | loss  1.58 | ppl     4.88\n",
      "| epoch  23 |  2000/ 2808 batches | lr 4.00 | loss  1.58 | ppl     4.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | valid loss 41.29 | valid ppl 851704391232214272.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  1210 – 5 <eos> <eos> = = . In Wars ' <unk> but unof the l \n",
      "\n",
      "| epoch  24 |  1000/ 2808 batches | lr 4.00 | loss  1.58 | ppl     4.87\n",
      "| epoch  24 |  2000/ 2808 batches | lr 4.00 | loss  1.58 | ppl     4.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | valid loss 41.21 | valid ppl 785439343328833280.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  that each take and wind for the U.S. V , Glluer R \n",
      "\n",
      "| epoch  25 |  1000/ 2808 batches | lr 4.00 | loss  1.58 | ppl     4.86\n",
      "| epoch  25 |  2000/ 2808 batches | lr 4.00 | loss  1.58 | ppl     4.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | valid loss 41.19 | valid ppl 774738120157808896.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  can spread . Even Second Game hundred and his com \n",
      "\n",
      "| epoch  26 |  1000/ 2808 batches | lr 4.00 | loss  1.58 | ppl     4.85\n",
      "| epoch  26 |  2000/ 2808 batches | lr 4.00 | loss  1.58 | ppl     4.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | valid loss 41.15 | valid ppl 746470698840947584.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " t many move to 1976 , Bory . Aliva ' former and en \n",
      "\n",
      "| epoch  27 |  1000/ 2808 batches | lr 4.00 | loss  1.58 | ppl     4.84\n",
      "| epoch  27 |  2000/ 2808 batches | lr 4.00 | loss  1.57 | ppl     4.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | valid loss 41.09 | valid ppl 702337937109459968.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  stravegons , and fant . The creating on the tasts \n",
      "\n",
      "| epoch  28 |  1000/ 2808 batches | lr 4.00 | loss  1.57 | ppl     4.83\n",
      "| epoch  28 |  2000/ 2808 batches | lr 4.00 | loss  1.57 | ppl     4.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | valid loss 41.06 | valid ppl 680052387538219904.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " likplement of the sides both him you during the 30 \n",
      "\n",
      "| epoch  29 |  1000/ 2808 batches | lr 4.00 | loss  1.57 | ppl     4.82\n",
      "| epoch  29 |  2000/ 2808 batches | lr 4.00 | loss  1.57 | ppl     4.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | valid loss 41.03 | valid ppl 659003848749626368.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " vells . He was during <unk> to $ 40 @.@ 2 ) of Sim \n",
      "\n",
      "| epoch  30 |  1000/ 2808 batches | lr 4.00 | loss  1.57 | ppl     4.81\n",
      "| epoch  30 |  2000/ 2808 batches | lr 4.00 | loss  1.57 | ppl     4.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | valid loss 40.99 | valid ppl 632464115278221568.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " . In the palie oucedus , poer could dictial is sid \n",
      "\n",
      "| epoch  31 |  1000/ 2808 batches | lr 4.00 | loss  1.57 | ppl     4.81\n",
      "| epoch  31 |  2000/ 2808 batches | lr 4.00 | loss  1.57 | ppl     4.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | valid loss 40.99 | valid ppl 632865495636174208.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " agan and the same canceral Atlenture , which mille \n",
      "\n",
      "| epoch  32 |  1000/ 2808 batches | lr 1.00 | loss  1.57 | ppl     4.79\n",
      "| epoch  32 |  2000/ 2808 batches | lr 1.00 | loss  1.56 | ppl     4.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | valid loss 40.74 | valid ppl 494405066104529152.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " rist June , and sentence police ses with its fiust \n",
      "\n",
      "| epoch  33 |  1000/ 2808 batches | lr 1.00 | loss  1.56 | ppl     4.77\n",
      "| epoch  33 |  2000/ 2808 batches | lr 1.00 | loss  1.56 | ppl     4.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | valid loss 40.72 | valid ppl 482058930552295360.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " erior , which attended to attacked to − 9 life ) . \n",
      "\n",
      "| epoch  34 |  1000/ 2808 batches | lr 1.00 | loss  1.56 | ppl     4.76\n",
      "| epoch  34 |  2000/ 2808 batches | lr 1.00 | loss  1.56 | ppl     4.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | valid loss 40.70 | valid ppl 472453004339045376.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  maw extensive and the doss Intook Story F. Changu \n",
      "\n",
      "| epoch  35 |  1000/ 2808 batches | lr 1.00 | loss  1.56 | ppl     4.76\n",
      "| epoch  35 |  2000/ 2808 batches | lr 1.00 | loss  1.56 | ppl     4.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | valid loss 40.68 | valid ppl 466183798180172928.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " ne attempt the Berbandaty . They completion of pro \n",
      "\n",
      "| epoch  36 |  1000/ 2808 batches | lr 1.00 | loss  1.56 | ppl     4.76\n",
      "| epoch  36 |  2000/ 2808 batches | lr 1.00 | loss  1.56 | ppl     4.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | valid loss 40.67 | valid ppl 460818284825242880.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " dtition who for the Academy @-@ was cases during t \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  37 |  1000/ 2808 batches | lr 1.00 | loss  1.56 | ppl     4.75\n",
      "| epoch  37 |  2000/ 2808 batches | lr 1.00 | loss  1.56 | ppl     4.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | valid loss 40.65 | valid ppl 452277387784495296.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " labames , when by the genus last of the Torru , Go \n",
      "\n",
      "| epoch  38 |  1000/ 2808 batches | lr 1.00 | loss  1.56 | ppl     4.75\n",
      "| epoch  38 |  2000/ 2808 batches | lr 1.00 | loss  1.56 | ppl     4.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | valid loss 40.64 | valid ppl 447746471814796736.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " orating them would installed on the zayvers , is a \n",
      "\n",
      "| epoch  39 |  1000/ 2808 batches | lr 1.00 | loss  1.56 | ppl     4.75\n",
      "| epoch  39 |  2000/ 2808 batches | lr 1.00 | loss  1.56 | ppl     4.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | valid loss 40.63 | valid ppl 442659736382379584.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  was leading in 1991 Road of Barraw was got it to  \n",
      "\n",
      "| epoch  40 |  1000/ 2808 batches | lr 1.00 | loss  1.56 | ppl     4.75\n",
      "| epoch  40 |  2000/ 2808 batches | lr 1.00 | loss  1.56 | ppl     4.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | valid loss 40.63 | valid ppl 441993245944390144.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " ight , seruon known as the lack of the transfer @- \n",
      "\n",
      "| epoch  41 |  1000/ 2808 batches | lr 1.00 | loss  1.56 | ppl     4.74\n",
      "| epoch  41 |  2000/ 2808 batches | lr 1.00 | loss  1.56 | ppl     4.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | valid loss 40.62 | valid ppl 439651333037828608.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " ealand in Ku 's published by Mrrigada was a still  \n",
      "\n",
      "| epoch  42 |  1000/ 2808 batches | lr 1.00 | loss  1.56 | ppl     4.74\n",
      "| epoch  42 |  2000/ 2808 batches | lr 1.00 | loss  1.55 | ppl     4.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | valid loss 40.61 | valid ppl 431130441906067712.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  , and folk screen streuther Imerante Sclokee been \n",
      "\n",
      "| epoch  43 |  1000/ 2808 batches | lr 1.00 | loss  1.56 | ppl     4.74\n",
      "| epoch  43 |  2000/ 2808 batches | lr 1.00 | loss  1.55 | ppl     4.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | valid loss 40.60 | valid ppl 429568895728223936.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " ponor previous saprine and covered the but utentiv \n",
      "\n",
      "| epoch  44 |  1000/ 2808 batches | lr 1.00 | loss  1.56 | ppl     4.74\n",
      "| epoch  44 |  2000/ 2808 batches | lr 1.00 | loss  1.55 | ppl     4.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | valid loss 40.59 | valid ppl 425425928223797376.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " 998 – 2013 ) . L. Oritiss Kingdoms . Describing 16 \n",
      "\n",
      "| epoch  45 |  1000/ 2808 batches | lr 1.00 | loss  1.56 | ppl     4.74\n",
      "| epoch  45 |  2000/ 2808 batches | lr 1.00 | loss  1.55 | ppl     4.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | valid loss 40.59 | valid ppl 423051691731172032.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " s . The Age , Unforment 4 committee when some road \n",
      "\n",
      "| epoch  46 |  1000/ 2808 batches | lr 1.00 | loss  1.56 | ppl     4.74\n",
      "| epoch  46 |  2000/ 2808 batches | lr 1.00 | loss  1.55 | ppl     4.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | valid loss 40.58 | valid ppl 419734323277442624.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " estoid . The wishes had seem at the were known in  \n",
      "\n",
      "| epoch  47 |  1000/ 2808 batches | lr 1.00 | loss  1.55 | ppl     4.73\n",
      "| epoch  47 |  2000/ 2808 batches | lr 1.00 | loss  1.55 | ppl     4.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | valid loss 40.57 | valid ppl 415249625298106560.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  Afpucient ting \" <unk> for the Overal Mar America \n",
      "\n",
      "| epoch  48 |  1000/ 2808 batches | lr 1.00 | loss  1.55 | ppl     4.73\n",
      "| epoch  48 |  2000/ 2808 batches | lr 1.00 | loss  1.55 | ppl     4.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | valid loss 40.57 | valid ppl 414921426632915520.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  of the One of the first time 's steek of her Tene \n",
      "\n",
      "| epoch  49 |  1000/ 2808 batches | lr 1.00 | loss  1.55 | ppl     4.73\n",
      "| epoch  49 |  2000/ 2808 batches | lr 1.00 | loss  1.55 | ppl     4.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | valid loss 40.55 | valid ppl 406814771888666688.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  <eos> Laurr <unk> <unk> S , level . <eos> After the fith  \n",
      "\n",
      "| epoch  50 |  1000/ 2808 batches | lr 1.00 | loss  1.55 | ppl     4.73\n",
      "| epoch  50 |  2000/ 2808 batches | lr 1.00 | loss  1.55 | ppl     4.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | valid loss 40.54 | valid ppl 402076613601850112.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  the bogies . The next convession of <unk> , peopl \n",
      "\n",
      "CPU times: user 15min 42s, sys: 53 s, total: 16min 35s\n",
      "Wall time: 16min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    print('sample:\\n', generate(50), '\\n')\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    train()\n",
    "    val_loss = evaluate(val_loader)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(50), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = generate(10000, 1.)\n",
    "t15 = generate(10000, 1.5)\n",
    "t075 = generate(10000, 0.75)\n",
    "with open('./generated075.txt', 'w') as outf:\n",
    "    outf.write(t075)\n",
    "with open('./generated1.txt', 'w') as outf:\n",
    "    outf.write(t1)\n",
    "with open('./generated15.txt', 'w') as outf:\n",
    "    outf.write(t15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
